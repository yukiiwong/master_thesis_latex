\chapter{针对多智能体的模式与出发时间选择方法}
\label{chp:bib}

\section{基于聚类的深度强化学习方法}

在前一节中，我们详细阐述了DQN算法作为解决JTMDTC问题的基本解决方案。然而，如何将该算法推广到解决具有大量个体的相同问题仍然是一个开放性问题。我们先前已经讨论过，存储所有个体的经验进行训练是计算上不明智且低效的，而随机选择一个或几个个体是不够的且不可靠的。因此，我们提出了一种优雅而有效的方法来获取代表性个体，以进行高效的模型训练，即基于个体的出行特征进行聚类。对于处于同一聚类中的个体，我们认为它们的出行特征相似。因此，它们中的每一个都可以被视为该聚类的代表，其经验可以代表其余个体来训练DQN。通过这种方式，我们不仅避免了部署与个体数量相同数量的代理，而且还有效地利用代表性个体的经验进行充分的模型训练。事实上，采用所提出的方法可以有效地解决具有许多个体的JTMDTC问题，而不会在决策制定中牺牲太多的最优性。我们将在结果中提供支持性证据。



\subsection{DBSCAN聚类方法}
DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类算法，旨在识别高密度区域，并将它们作为簇的核心。DBSCAN不需要假设簇的数量，因此是一种非参数方法。它可以自适应地调整簇的大小和形状，并在存在噪声的情况下保持较好的性能。

DBSCAN的核心思想是将数据点分为三类：核心点、边界点和噪声点。核心点是一个密度可达的点集合，即它周围的密度大于等于指定的阈值。边界点是一个密度可达的点，但其周围的密度小于指定的阈值。噪声点则是不属于任何簇的点，它们周围的密度也小于指定的阈值。

除了DBSCAN算法，还有其他一些聚类算法，例如$k$-means和层次聚类。$k$-means算法是一种常用的聚类算法，它通过将数据集划分为$k$个簇来进行聚类。它的核心思想是将数据点分配到距离其最近的质心（簇的中心），并将质心更新为簇中所有点的平均值。然后，重复这个过程直到质心不再变化或达到预定的最大迭代次数。

层次聚类是一种自下而上的聚类方法，它通过递归地将最相似的数据点组合成更大的簇，最终形成一个完整的聚类树。层次聚类可以是聚合的（自底向上）或分裂的（自顶向下）。在聚合层次聚类中，每个数据点开始时都是一个单独的簇，然后将最相似的簇合并在一起，直到所有数据点都被分配到一个簇中。在分裂层次聚类中，开始时将所有数据点都分配到一个大簇中，然后逐步将其分裂成较小的簇，直到达到预定的聚类数目。

尽管$k$-means和层次聚类算法也可以用于代表性个体的选择，但它们对噪声点的处理方式不如DBSCAN算法，可能会将噪声点误分类为一个簇或将它们分配到多个簇中。此外，DBSCAN算法可以自动确定簇的数量，并且不需要提前指定$k$或层次聚类的高度。

在本研究中，我们选择了DBSCAN算法作为代表性个体的选择算法，因为它能够很好地处理数据噪声和密度不均匀的情况，并且不需要提前指定簇的数量。通过聚类选择代表性个体，我们可以大大减少强化学习算法中状态和动作空间的规模，并提高训练效率。


\subsection{聚类参数的选择}
为了获得代表性个体，我们采用了一种广泛使用的聚类算法，称为具有噪声的基于密度的空间聚类（DBSCAN）\citep{ester1996density}。在使用DBSCAN算法时，有两个参数需要选择：邻域半径（$\varepsilon$）和最小样本数（$m$）。

邻域半径决定了聚类算法将哪些点归为同一簇。若两个点之间的距离小于邻域半径，则认为这两个点属于同一簇。最小样本数表示在邻域半径内必须有至少$m$个样本才能被归为一簇。这两个参数的选择将直接影响聚类结果的质量，因此需要进行谨慎的选择。

在本研究中，我们选择了经验法来选择DBSCAN算法的参数。经验法是指依赖于经验或常识的方法，通常用于缺乏理论基础或无法用数学方法明确解决的问题。我们通过手动调整邻域半径和最小样本数来找到最佳参数组合。

我们首先尝试了不同的邻域半径和最小样本数的组合，并记录每个组合的轮廓系数得分。轮廓系数是一种用于评估聚类结果的指标，其值介于$-1$和$1$之间。其计算方法是将一个样本的簇内平均距离（$a$）与与其最近簇的所有样本的簇内平均距离（$b$）进行比较，计算得出该样本的轮廓系数为$\frac{b-a}{max(a,b)}$，并将所有样本的轮廓系数求平均。轮廓系数越接近1，说明样本聚类得越好，越接近-1，说明聚类效果差。

通过实验，我们发现最佳参数组合为邻域半径为$0.5$公里，最小样本数为$2$。在这个参数组合下，我们得到的轮廓系数为$0.82$，表明聚类效果良好。

总的来说，选择聚类算法的参数是一个关键的问题，需要结合经验和实验结果进行调整和优化。经验法是一种常用的方法，通过手动调整来选择最佳参数组合，但也需要注意参数的合理性和可重复性。

\subsection{深度强化学习模型的改进}
将定制的DQN与个体聚类和获取代表性的过程相结合，得到了解决具有许多个体的JTMDTC问题的最终集成算法。要训练的代理数量等于代表或群集的数量。这些代理与它们各自的存储池同时进行训练。一旦充分训练，它们就可以联合使用，为不同的个体做出出行选择决策，无需重新进行聚类。也就是说，选择获得最高奖励的代理所采取的行动来实施。

通过使用基于深度强化学习的仿真平台解决JTMDTC问题，我们的设计目标是提供一个高效，精确且可扩展的仿真工具，以便研究人员和政策制定者能够定量地评估出行模式和出行时间选择的不同策略。我们提出的集成算法通过聚类个体并获取代表性个体来训练代理，从而有效地减少了计算成本并提高了模型训练效率。这种方法不仅可以应用于JTMDTC问题，还可以应用于其他基于个体决策的问题，具有广泛的应用前景。

\section{模型的验证}

为了检验提出方法的优越性或最优性，我们进行了比较分析，将提出的方法与普通的DQN方法以及一种暴力方法进行比较，该方法会随机选择并尝试所有可能的行动。我们选择了50个不参与训练的网络中的测试个体的新的时变OD出行进行比较分析。为了进行比较，我们让每个测试个体根据这些不同决策制定者之一来执行操作，并收集结果奖励。对于暴力方法，执行1,000个周期以完全探索动作空间。

图\ref{com1}、\ref{com2}和\ref{com3}展示了三个选定的测试个体的比较结果。可以看到，暴力方法的奖励显著波动，这是由于随机选择的行动不总是保证良好结果。对于提出的方法和普通的DQN方法，获得的奖励是一个单一值，表示为一条直线，并与JTMDTC问题的最优解相关联。很明显，对于所有三个测试个体，提出的方法给出的解决方案不仅优于普通的DQN方法，而且还优于大多数暴力方法给出的解决方案。事实上，在不聚类个体并利用代表性的情况下，大约有一半的暴力方法给出的解决方案可以击败由普通DQN方法给出的解决方案（见图\ref{com1}和\ref{com2}）。

为了从全局角度看待所有50个测试个体的比较，我们找到每个测试个体由暴力方法获得的最大奖励，该奖励被视为JTMDTC问题的（近似）最优解。通过将提出的方法和普通DQN方法给出的解决方案与这个参考值进行比较，我们可以观察到性能差异。如图\ref{com4}所示，提出的方法给出的大多数解决方案（超过50个中的40个）都超过参考值的95％，这意味着这些解决方案接近最优。这对于普通的DQN方法显然不是这样，因为只有约30％的解决方案超过了相同参考值的95％，更不用说还有一些解决方案低于参考值的60％。因此，比较结果表明，在应用于解决具有许多个体的JTMDTC问题时，提出的方法的有效性，以及代表性在完成此任务中发挥的重要作用。由于测试个体不是训练的一部分，因此结果表明提出方法具有很好的可转移性。

\subsection{与传统方法的对比}

现在我们试图考察在信息不完全的情况下，所提出的方法的性能。正如之前所讨论的，部分信息从人类行为学的角度更具相关性，因此预计会导致更差的行动选择。在此，我们还考虑了另外两种模型，用于比较。第一个是一阶马尔可夫链（MC）模型，仅使用当前旅行距离和出发时间差来决定下一个选择。其状态、转移和初始状态概率以及奖励函数均源自DRL模型。两者的主要区别在于决策过程。MC模型使用转移和初始状态概率来模拟随时间变化的行为，而DRL模型使用迭代试错过程。

第二个是传统的MNL模型，使用奖励函数（式（\ref{reward function}））。我们使用在信息完全的情况下，由提出的方法得到的个体出行选择作为基准线，根据其来比较其他模型的性能。考虑了三个性能评估和比较指标。除了奖励之外，另外两个是负对数损失（NLL）和Jaccard指数。这两个指标都衡量其他模型产生的行动与基线获得的行动之间的接近程度或相似程度。因此，它们可以反映其他模型相对于基线的优化水平。但需要注意的是，NLL的较低值是期望的，而对于Jaccard指数，值越高越好。

表\ref{eva}总结了三个性能指标的比较结果。如预期的那样，在信息完全的情况下，所提出的方法提供了实现尽可能多的奖励的最佳性能。所有其他模型的性能都较差，通过比较平均奖励值就能看出这一点，这些值都低于基线的奖励值。这种趋势也适用于NLL和Jaccard指数。尽管如此，在部分信息的情况下，所提出的方法仍然表现出比一阶MC模型和MNL模型略好的性能，这表明即使存在部分信息，所提出的方法仍然是有效的。

\subsection{模型参数的灵敏性分析}

我们现在进行两个敏感性分析，以研究所提出方法在模型参数变化时的性能变化。第一个参数是代表数量，第二个参数是训练个体的集合。为了看到前者的影响，我们进行了进一步的实验，分别使用1、10、20和40个代表。我们保持相同的实验设置，将60个个体的时间依赖OD行程聚类成上述数字，以选择训练代表，而其他50个测试个体则用于评估和比较。同样，蛮力方法作为参考。

比较结果总结在表\ref{cluster_size}中。由于内存溢出，40个代表的实验无法在同一台机器上完成，因此没有报告结果。随着代表数量的增加，所需的训练或计算时间增加，这是预期的。然而，代表数量的增加确实会导致更好的奖励。将一个代表转变为四个代表，奖励得到了最大的提高。进一步将该数字增加到10或20并不能显著提高奖励。这个结果表明，增加代表数量不一定划算。实际上，少量代表已经可以在合理的计算时间内产生相当好的结果。使用蛮力方法得到的最大奖励作为参考值，我们比较了所提出的方法所给出的奖励高于参考值95％以上的测试个体数量。如预期的那样，对于4、10和20个代表，这个数字保持较大且变化很小。图6进一步显示了对于不同数量的代表，四个选定测试个体结果的比较。只有一个代表显然不足以击败蛮力方法，而四个或更多代表则产生了有希望的结果。

为了显示所提出方法的性能并不因训练个体的不同而发生显著变化，我们使用不同的训练个体集合进行另一组实验，使用四个代表对DQN进行训练，其余的实验设置保持不变。表\ref{cluster_agents}总结了这样四个实验的结果。由于不同的训练个体集合不会改变计算时间（对于四个代表，计算时间为22小时），因此不再报告这个度量。从结果中可以看出，所提出方法的性能是稳定的，不会因为用于训练的代表个体不同而表现出显著的变化。类似于图\ref{appendixa1}，图\ref{appendixb1}显示了当使用不同的训练个体集合时，四个选定测试个体结果的比较，表明所提出的方法对代表个体的选择具有鲁棒性。这些结果表明，所提出的方法是有效的，不会受到训练代表个体集合的影响。从这些敏感性分析中可以得出结论，所提出的方法在实际应用中具有很强的可操作性和鲁棒性。通过将模型应用于新的时间依赖 OD 数据集并比较与传统模型和暴力方法的性能，我们证明了该方法在解决 JTMDTC 问题方面的有效性。
