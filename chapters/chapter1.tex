\chapter{绪论}
\label{chp:installation}


\section{研究工作的背景及意义}









城市旅行需求在过去几十年里由于人口和城市化的快速增长而急剧上升。增长速度远超过了交通基础设施的扩展。需求与供应失衡的后果就是全球范围内普遍存在的交通拥堵，这也说明了各种旅行需求管理策略的出现和必要性。然而，这些策略的成功实施在很大程度上取决于如何理解和建模出行者的出行选择。为了获得准确的出行需求预测并实施有效的需求管理策略，研究人员和政府机构了解出行者在出行时如何进行决策将会至关重要。一旦决策者知道出行者在何时何地以及将采取什么模式出行，就可以提供有效的解决方案来缓解拥堵。因此，出行决策建模成为交通研究的关键。

研究人员通常将出行决策描述为不同维度的备选方案选择，例如出发时间、目的地、方式和路线。这些选择问题通常被描述为离散或者连续选择模型。早期的出行决策模型只考虑了一个维度，即从该选择维度的一组相互排斥的备选方案中选择了一个备选方案。然而在实际生活中，需要结合不同行为维度的进行多维决策才足以支持日益增长的拥堵管理策略应用。与传统的单维度出行选择模型不同，多维度模型考虑了不同选择之间的相关性。与模式和出发时间相关的两个关键且相关的选择从微观角度来看，这两个选择反映了个体对其出行的偏好。从宏观角度来看，它们决定了交通网络的时空旅行需求。我们强调，交通方式的吸引力以及其可能的选择取决于其服务水平。这种服务水平可能受到诸如拥堵定价、公共交通优先、各种类型激励等众多政策措施的影响。因此，为了评估这些政策措施，有必要建立一个同时考虑出行方式和出发时间选择的建模框架。



大多数现有的关于建模联合出行方式和出发时间选择（JTMDTC）的研究都是使用不同类型的离散选择模型（DCM），主要寻求随机效用最大化。特别是，考虑到它们在描述不同选择替代方案之间的相关性方面的能力，通常使用诸如多项式logit（MNL）、嵌套logit（NL）、交叉嵌套logit（CNL）等模型。利用随机效用最大化的模型依靠着其强大的理论依据而被广泛地应用。基于随机效用的模型是可以解释出行选择的基本理论，而对于复杂的决策过程建模的适用性，尤其是在选择预测中，可能会受到随机效用函数中线性结构的限制。对于多维选择问题，不同维度之间的关联结构也需要预先确定。尽管这些模型在理论上是解决旅行选择问题的典型解决方案，但在复杂决策过程中，由于随机效用函数的表述限制，它们的适用性可能受到限制。由于缺乏适应性以及出行者对旅行信息的不完美感知，由离散选择模型所推导出的出行选择可能不一定会导致最佳结果。

效用的随机成分不仅可以解释出行者对与观察信息的局限性，而且可以考虑决策者的不完全信息和偏好的随机变化。然而，以下事实支持了对基于学习方法的出行选择模型的需求。首先，乘客在模式选择的决策过程，是由不同出行方式的服务水平信息告知和指导的。这些知识通常是通过各种方式获得的(包括出行经验)，并且会随着时间动态变化。第二，出行决策受到一些行为因素的影响，其中乘客更倾向于(更少)选择(改变)他们已经习惯的模式。第三，交通系统的随机性和时间依赖性最可能引起出行者的自适应模式切换决策，在这种决策中，出行者可能会根据以往的经验更新他们对每种出行模式的预期效用。传统方法不能解决决策过程中涉及的时间维度。因此，与传统的选择建模方法相比，基于学习的出行决策模型更可取。

近年来，深度强化学习已成为应对复杂决策问题的关键机器学习方法之一，原因在于它在复杂环境中具有较强的学习能力。这种学习能力正是传统离散选择模型所缺乏的，可以充分用于出行选择建模或出行推荐。不同交通方式服务水平的时变信息可以从日常出行中获得的经验中获取，基于此，出行者可以按照日常方式学习并调整他们的出行选择。这种内在的学习和决策过程可以表示为一个马尔可夫决策过程，并由深度强化学习解决。实际上，它可以视为一个推荐问题，目标是为个体实现尽可能高的旅行效率。出行选择的决策过程是一个复杂的过程，会受到环境的影响而不断地变化，通过建立传统的出行选择模型来解释出行行为的方法过于理想化。而此类场景很好地契合了强化学习“无模型、自学习、数据驱动”，使用强化学习的方法可以将此类复杂的模型使用深度神经网络进行描述，提取不同外界环境的特征数据如等待时间、出行成本等构建状态输入，再对出行者的出行行为进行优化，利用大数据训练网络增加其真实性和可靠性。相较于传统的离散选择模型，强化学习的方法对复杂的场景适应能力有极大的提升，并且适用的场景更加广泛。因此，在本文中，我们利用DRL的优势，提出了一种新的JTMDTC模型，以最大化个体在动态多模态交通网络中的出行效用。为了实现计算效率并实现大规模应用，我们在建模框架中嵌入了聚类方法。通过精心设计学习结构和输入，我们证明了所提出的方法能够在复杂的交通环境中为个体提供更好的出行选择，实现更高的效用。


\section{国内外研究}


在出行选择的模型中，通常使用基于随机效用的离散选择模型对不同维度的选择行为进行建模。从McFadden\cite{mcfadden1973conditional}在1973年提出了著名的Multinomial Logit（MNL）模型用于行为选择建模以来，Logit系列模型就被广泛应用于出行决策问题。然而，MNL存在一个被公认的问题：它假设了不相关的替代方案的独立性，也被称为IIA（independence of irrelevant alternatives）特性。这表示替代方案未观察到的特征彼此之间相互独立，然而在一些出行选择的问题中这个假设将会不成立。例如，在离散出发时间选择中，相邻出发时间区间的未观测特征往往表现出显著的相关性。为了解决这一问题，Ben-Akiva[5]等在1998年提出了Nested Logit（NL）模型和有序广义极值模型（Ordered Generalized Extreme Values, OGEV）。NL模型能够识别嵌套组内不同替代方案之间的相关性。有序广义极值模型允许为每一对分组的备选方案提供一个相关参数。经过Bhat[6-8]在1998年的测试，得出的结论是，NL和OGEV模型的性能都优于MNL。在此之后，不同的研究人员针对问题的多样性提出了更先进的NL模型，如Ben-Akiva和Bierlaire [9] 在1999年提出的cross-nested Logit（CNL）模型和Lemp[10]等在2010年提出的连续CNL模型。另一种改进的离散选择模型是De Jong 等在2003年提出的mixed Logit(MMNL)模型\cite{de2003model}，它通过改变MNL模型的参数随给定分布变化来考虑个体之间的异质性。然而，MMNL的一个限制是，它需要对整个人口的参数分布进行特定的假设。这种限制可以通过潜在类(LC)模型来解决，该模型可以通过将总体划分为离散数量的类来捕获未观察到的偏好异质性\cite{fukuda2010semiparametric}。

另一种研究出行决策的主流方法是机器学习。与统计方法不同，在统计方法中，研究人员试图确定模型结构和需要估计的参数，机器学习方法关注的是数据本身，并试图找到不同参数之间的关联。相较于随机效用的模型，机器学习模型的结构更加灵活，方便其探索不同特征之间的关联。针对出行决策的建模，主要有以下几种主流的机器学习方法：决策树模型[11]，神经网络模型[12]，以及支持向量机[13]。与随机效用离散选择模型相比，这些机器学习方法可以处理大型数据库。然而，机器学习方法很少能捕捉到对出行行为研究较为重要的因素，包括时间价值(VOT)和弹性。此外，使用机器学习方法作为模型的主要框架还存在一个限制是机器学习模型对训练数据很敏感，在样本不足或有偏倚的情况下，会导致欠拟合或过拟合问题。

强化学习作为一种被广泛应用的学习机制,是利用环境的反馈评价作为学习的输入,学习主体拥有较强的环境适应能力的机器学习方法,因此适用于重复日变的交通决策场景中。强化学习被用来解决各种领域的顺序决策问题，如机器人控制、电子游戏和系统优化等。强化学习的理论为人类行为提供了可解释的心理学和神经科学视角，即人类如何在给定的环境中计划自己的行为。此外，强化学习框架提供了智能决策的数学形式化形式，在智能体控制中具有强大而广泛的适用性，可直接应用于控制理论中顺序决策问题的求解。在交通领域，强化学习方法也受到了广泛的应用，例如交通流管理、自动驾驶，以及路线规划[14]。近期，一些研究已经采用强化学习方法来建模出行者日常活动计划以及出行决策。

现有的出行决策与出行需求预测的研究工作多使用基于价值的强化学习方法，Janssens[15]在2007年使用Q-learning的强化学习方法解决活动调度问题。Vanhulsel[16]等在2009年通过基于Q-learning的方法构建MATSim结构方程模型。Medhat[17] 等在2008年开发了一个更全面的动态公共交通路径和出行活动选择模型，称为MILITRAS系统，其中的模型使用了预先设定的奖励(效用)函数。

近几年，深度强化学习在控制复杂智能体的决策行为上取得了巨大的成功，并将强化学习算法与许多神经相关因素的研究相结合，激发了大量使用人工神经网络作为通用函数逼近器的强化学习方法的研究。Hausknecht[18]等在2016年发表的著作研究了使用深度强化学习方法与多智能体合作行为。值得注意的是，它将多智能体研究中的矩阵博弈推广到更复杂的状态和行动空间。


\section{本文的贡献与创新}

在使用强化学习的仿真环境中，可以根据不同出行场景将出行者主要分成两种：有电子地图导航和无电子地图导航。在有电子地图导航的场景中，出行者信赖电子地图导航，会根据导航信息一般选择行程最短的模式和路径行驶。在此场景中，出行模式选择问题将转变为备选路径的行程时间预测和预估价问题。可以考虑使用预计到达时间（ETA）的计算方法解决[22]。在无电子地图导航的场景中，出行者只能依靠自身过往经验，根据经验记忆选择效用最大的出行模式和路径。这种场景下，出行者的每次决策都会得到环境带来的不同反馈，与强化学习的思想相契合。因此这种场景可以使用强化学习的模型解决。

相较于传统的离散选择模型，强化学习存在以下三点优势：

1.强化学习模型直接与环境交互，减少了传统离散选择模型的假设限制。传统离散选择模型需要对环境的条件预先假设并检验，在复杂多变的环境下传统模型的弊端将会体现。

2.强化学习的模型会减少采集数据的成本。一项新的交通政策在实施前需要大量的仿真验证，传统模型需要采集大量的现实数据来验证模型的有效性。强化学习可以基于智能体已知的场景，通过更改仿真环境中的基础设施或策略，使得智能体学习处理未知场景下的决策行为。

3.考虑智能体记忆能力，贴近实际决策过程。在强化学习的模型中，智能体做出动作后会根据以往经验以及自身的探索不断优化不同决策行为的价值及策略，这与实际中人在进行决策时的惯性一致。


\section{论文的结构安排}