\chapter{基于深度强化学习的出行模式与时间选择方法}
\label{chp:float}

强化学习是一种通过迭代地改进策略来最大化累计奖励或回报的机器学习方法。在应用强化学习到具有马尔可夫属性的序列决策过程中，需要先构建一个马尔可夫决策过程，该过程定义了环境的演变，考虑到强化学习代理所采取的行动。强化学习代理通过行动探索和开发不断地与环境互动，根据当前状态 $s_t$ 进行行动。每次行动会使环境演变成一个新状态 $s_{t+1}$，并获得相应的奖励 $r_t$，反馈给代理以改善其决策逻辑。这个过程一直迭代，直到代理成功学习到一个能够最大化累计奖励的策略 $\pi$，也就是一个决策者。因此，强化学习的关键在于根据奖励不断迭代改进策略。

在本研究中，将每个出行者视为具有学习能力的智能实体，通过马尔可夫决策过程来建模每个出行者跨越多个连续日的交通出行行为。每个出行者能够选择的行动包括不同组合的出行方式和出发时间。最终由个人采取的行动 $a_t$ 决定了环境演变到的下一个状态 $s_{t+1}$。该状态应反映个人关于行程本身以及相关环境的最新知识。选择此行动所获得的奖励 $r_t$（与旅行成本相关）有助于改善个人的决策逻辑，这样个人就能逐渐学习到最优的行动策略，并最大化累计奖励。
\section{马尔可夫决策过程框架}
正如先前所讨论的那样，马尔可夫决策决策过程是建模和优化出行模式和时间选择的先决条件。从数学角度来说，它是一个五元组$(S, A, P, R, \gamma)$，其中$S$表示状态空间，$A$表示动作空间，$P$表示状态转移概率，$R$表示奖励函数，最后$\gamma$是折扣因子。在这里，智能体是出行模式和时间选择中个人的决策者。虽然将每个个体视为智能体在概念上是有效的，但是由于代理数量等于个体数量，所产生的计算复杂性是大规模应用的主要障碍。从推荐系统的角度来看，研究适用于所有个体的常见决策逻辑是值得探究的，这反映了该方法的普适性。接下来，我们将进一步阐述如何构建和解决问题特定的马尔可夫决策决策过程。

\subsection{动作空间}

动作空间是强化学习中的一个关键概念。在建模动作空间时需考虑出行者的个性化特征。例如，不同出行者对于出行方式和出发时间的偏好不同，因此他们的动作空间也会不同。对此，可以引入个性化因素对动作空间进行建模。例如，可以考虑出行者的年龄、性别、职业、家庭状况等因素，进一步细化动作空间的描述，提高模型的预测能力和适应性。

此外，动作空间的大小和粒度也会影响到模型的性能和可解释性。如果动作空间过大，模型的训练和预测会变得非常困难，同时也会增加模型的计算复杂度和存储空间需求。而如果动作空间过小，模型的表达能力就会受到限制，无法对真实情况进行有效建模。因此，需要在合理范围内对动作空间进行定义和限制，以平衡模型的性能和可解释性。在实际应用中，建模动作空间的过程也需要考虑到数据的可用性和质量。例如，在收集出行调查数据时，需要尽可能全面和准确地收集出行者的出行方式和出发时间等信息，以便更好地建模动作空间和进行模型预测。同时，在对数据进行预处理和清洗时，也需要对动作空间进行适当的定义和筛选，以避免噪声和异常数据对模型的影响。

在JTMDTC中，动作空间包括出行方式和出发时间两个方面。在选择出发时间时，出行者需要考虑到交通拥堵、出行时间和其他因素对行程的影响。例如，在高峰期出发可能需要更长的旅行时间，而在非高峰期出发可能可以更快地到达目的地。因此，在建模动作空间时，需要综合考虑各种因素，以便在代理决策时提供准确的信息。出行方式可以是私家车、公共交通或自行车。在公共交通中，可以选择乘坐公交车或地铁，但是不考虑三种交通方式之间的换乘。这是因为换乘涉及到很多变量，比如停车地点、换乘时间等，需要更多的研究来处理。

对于出发时间，每个出行者都有一个初始或期望出发时间$t_0$。但是，由于各种原因，可能需要调整出发时间，比如交通拥堵、天气等。在JTMDTC中，出发时间可以在一个时间窗口$[t_{\min},t_{\max}]$内进行调整。这个时间窗口是由最早和最晚的出发时间$t_{\min}$和$t_{\max}$确定的。出发时间的调整是以离散间隔为单位进行的，而不是连续方式进行。这是因为在实际交通中，时间通常是以分钟为单位进行的，而不是连续的时间。

对于上述所提到的交通方式和出发时间，需要进行适当的编码以便于代理在模型中进行操作。在本文中，采用离散化编码的方式，将交通方式和出发时间分别离散化为一组离散的选项。例如，对于交通方式，可以将私家车、公交车、地铁和自行车分别编码为$m_1$、$m_2$、$m_3$和$m_4$。对于出发时间，可以将$t_0$和时间窗口$[t_{\min},t_{\max}]$离散化为一组时间步长，例如每5分钟一步。这样，代理可以从一组离散的选项中进行选择，并决定最佳的出行方式和出发时间。动作空间的描述采用了向量的形式。向量$\bm{a}$包括交通方式$\tilde{m}$和出发时间$\tilde{t}$。交通方式可以是可用交通方式$m_{1},m_{2}, \ldots, m_{N}$中的任意一种，而出发时间必须在时间窗口$[t_{\min }, t_{\max }]$内。因此，动作空间可以表示为：
\begin{equation}
\bm{a}=\left[\begin{array}{c}
\tilde{m} \\
\tilde{t}
\end{array}\right]=\left[\begin{array}{c}
\text { 出行模式 } \\
\text { 出发时间 }
\end{array}\right]
\in\left[\begin{array}{c}
\left\{m_{1},m_{2}, \ldots, m_{N}\right\} \\
{\left[t_{\min }, t_{\max }\right]}
\end{array}\right],
\end{equation}


动作空间是模型中重要的一部分，它定义了代理能够采取的所有行动，直接影响着模型的效果和性能。在建模时，需要综合考虑多种因素，将交通方式和出发时间等重要信息进行适当的编码，以便于代理进行决策。
\subsection{状态空间}

状态空间定义了智能体选择行动的上下文环境。对于JTMDTC，状态空间被设计为不仅包含有关行程的最新知识，还包括智能体早期经验。这种状态空间设计在很大程度上类似于理性人类的决策机制，即从经验中学习。由于重点不在于经验选择建模而在于选择指导或推荐，因此我们假设智能体能够充分感知环境，因此对行程具有完全信息。然而，我们稍后将讨论这种假设可能会有所放松。

行程信息首先包括每种交通方式的旅行距离$L$和记忆旅行时间$\bar{T}$。前者是模式$m$的旅行距离，而后者是模式$m$的平均经验旅行时间。其原因有两个——利用经验和在交通随机性存在的情况下保持稳健性。作为行程信息的另外两个变量是初始出发时间$t_0$和出发时间差或偏移量$\Delta t$相对于$t_0$。将所有内容组合起来，得到以下特定于行程信息的状态向量：
\begin{equation}
\begin{aligned}
{\bm{s}_\text{trip}^m}=\left[\begin{array}{c}
L_m \\
{\bar{T}}_m \\
t_{0} \\
\Delta t
\end{array}\right]=&\left[\begin{array}{c}
\text { travel\;distance } \\
\text { memory\; travel\; time } \\
\text { initial\; departure\; time } \\
\text { departure\; time\; difference}
\end{array}\right].
\end{aligned}\label{equation:trip}
\end{equation}

环境信息基本上包括有助于不同交通方式旅行成本的因素。对于公共交通，考虑到两个因素，即可达性和票价。在这里，我们将可达性$p$定义为完成行程的第一和最后一段所需的总步行距离：

\begin{equation}
p=d_{\text{origin}}+d_{\text{destination}},
\end{equation}

其中$d_{\text{origin}}$和$d_{\text{destination}}$分别是从最近的公交车站或地铁站到起点和终点的步行距离。公共交通票价是必须支付的使用该服务的货币成本。对于私家车，我们考虑燃油价格作为影响因素，并将其放在状态中。因此，特定于环境信息的状态向量如下所示：

\begin{equation}
\begin{aligned}
\bm{s}_\text{reduced}=\left[\begin{array}{c}
\bar{T} \\
t_{0} \\
\Delta t
\end{array}\right]=&\left[\begin{array}{c}
\text { memory travel time } \\
\text { initial departure time } \\
\text { departure time difference }
\end{array}\right].
\end{aligned}\label{equation:obs}
\end{equation}

最后，我们要注意到，上述状态向量（或状态空间）中包含的变量可能因特定问题或应用而异。状态空间设计的目标是确保代理可以获得对其行动选择有意义的环境信息。对于某些应用程序，可能需要包括更多的环境因素，例如天气和道路条件。而对于其他应用程序，可能只需要少数几个变量即可获得有效的行动建议。因此，状态空间设计取决于特定的问题和应用场景。

总之，状态空间是强化学习中非常重要的一个概念，它定义了代理在决策时需要考虑的上下文环境。在设计状态空间时，我们需要仔细考虑应用场景和问题的特征，以确保状态空间中包含的信息能够有效地指导代理的决策。同时，我们也需要注意状态空间的维度和大小，以便使代理能够有效地处理状态，并且可以在有限的时间内完成状态的学习和更新。
\subsection{奖励函数}
在强化学习中，智能体的目标是通过最大化长期奖励来学习最优的决策规则。通过奖励函数，智能体可以计算每个动作对于实现这个目标的预期收益。在这个例子中，我们的奖励函数是旅行效用，旨在最小化总旅行费用。智能体将根据预期的长期奖励来选择动作，以便在未来的交互中最大化收益。

当智能体选择并执行一个动作时，会导致环境从当前状态转移到一个新状态。同时，智能体还会收到一个反馈或奖励，旨在改善智能体的决策逻辑。奖励可以是正面的，意味着动作是明智的，也可以是负面的，意味着相反。在这里，我们定义奖励为旅行效用，这主要由各种货币成本组成。具体来说，步骤i获得的奖励计算如下：

\begin{equation}
r_{i}=\frac{E_{1}-C_{m}^{i}}{E_{2}},\label{reward function}
\end{equation}

其中，$C_m^i$ 是交通方式$m$的总旅行费用，$E_1$ 和 $E_2$ 是映射和缩放成本到奖励的两个常数。总旅行费用又可以分解为三个部分，即总旅行时间$T_m^i$、行程延误$\delta(t^i)$ 和其他与旅行相关的成本$F_m^i$。

\begin{equation}
C_{m}^{i}=\alpha T_{m}^{i}+\delta\left(t^{i}\right)+F_{m}^{i},\label{costfunction}
\end{equation}

其中，$\alpha$ 是时间的价值。这个公式描述了在一个旅行过程中，奖励是如何被计算的，以及成本是如何被划分的。智能体可以通过调整其动作来优化它所接收到的奖励，并在行程中实现更好的效用。

只考虑总旅行时间的问题是忽略了实际到达时间。也就是说，尽管总的旅行时间很短，但到达时间可能与理想的时间相差甚远。因此，引入了计划延迟，这个概念可以追溯到\cite{small1982scheduling}。假设每个人都有一个期望的到达时间，早到和晚到都会产生一个所谓的日程延误成本。当实际到达时间偏离期望时间时，计划延迟成本会以线性方式增长。从数学上讲，它表示为：。
\begin{equation}
\delta(t^i)= 
\begin{cases}\beta\left(t+T_{m}-t_\text{d}\right) & \text { if\quad } t+T_{m}-t_\text {d}<0, \\ 
0 & \text { if\quad } t+T_{m}-t_\text {d}=0, \\ 
\gamma\left(t_\text {d}-t-T_{m}\right) & \text { if\quad } t+T_{m}-t_\text {d}>0,
\end{cases}
\end{equation}

其中，$\beta$和$\gamma$分别是早到和晚到的行程延误单价，$t_\text{d}$是期望到达时间。通过考虑行程延误成本，可以更准确地评估各种出行方式的效用。

除此之外，其他的与出行相关的费用主要是指汽车的燃料费用和公共交通的票价。对于私家车来说，燃料费用是与行驶距离成正比的，而对于公共交通来说，则是根据具体的交通工具的票价。因此，其他出行相关费用可以表示为公式：
\begin{equation}
F_{m}^i= 
\begin{cases}
L_\text{car} \cdot o  & \text { if\quad  } m=\text { car }, \\ 
f  & \text { if\quad  } m=\text { public\;transportation }, \\ 
0   &  \text { if\quad } m=\text {  cycling,}
\end{cases}
\end{equation} 

其中，$f$可以表示为：

\begin{equation}
f=\mathbb{I}(\text { bus }) \cdot f_{\text {bus }}+\mathbb{I}(\text { subway }) \cdot f_{\text {subway }},
\end{equation}

这里的$\mathbb{I}(\cdot)$是一个指示函数，当选择的交通工具是公共汽车或地铁时，它返回1，否则返回0。公交车费用$f_{\text {bus }}$是固定的，而地铁费用$f_{\text {subway} }$则随着行驶距离的增加而增加。这些费用可以用于计算奖励函数中的其他出行相关成本项。

因此，仅考虑总出行时间可能无法完全反映出行者的需求和偏好。因此，在交通规划和出行选择研究中，需要考虑更多的因素，如出行成本、出行时间安排的灵活性、出行方式对健康和环境的影响等。这些因素可以通过建立数学模型来加以考虑，并通过模型模拟和分析来确定最佳的出行方式和路线。这种模型和分析方法可以帮助交通规划者和出行者做出更明智的决策，同时也可以为交通管理部门提供有关公共交通需求和运营管理方面的有用信息，以提高城市交通系统的效率和可持续性。

\section{基于深度Q网络算法的模式与出发时间选择算法}

\subsection{神经网络结构设计}

The first step in designing a DQN model is to choose an appropriate neural network architecture.

The architecture should be capable of taking the state as input and producing Q-values for each action in the action space as output.

Convolutional neural networks (CNNs) are commonly used for problems with image-based inputs, while fully connected neural networks can be used for problems with vector-based inputs.

The number and size of layers in the network can also vary depending on the complexity of the problem.

\subsection{超参数的选择与标定}

Hyperparameters play a crucial role in the performance of a DQN model.

The learning rate, batch size, discount factor, and other hyperparameters should be carefully selected to optimize the performance of the model.

The learning rate controls the rate at which the model updates its weights during training.

The batch size determines how many state-action pairs are processed in each training iteration.

The discount factor determines the balance between immediate and future rewards in the Q-value calculation.

Other hyperparameters, such as the size of the replay buffer or the frequency of target network updates, can also have a significant impact on model performance.

\subsection{模型的优化}

Various optimization techniques can be used to improve the stability and convergence of a DQN model.

Experience replay is a technique where past experiences are stored in a buffer and sampled randomly during training.

Target networks can be used to estimate the Q-values of the next state to improve stability during training.

Prioritized experience replay can be used to sample more important experiences more frequently.

Reward shaping can be used to modify the reward function to incentivize certain behaviors.

The selection and use of these optimization techniques can depend on the specific problem and the network architecture used.

\section{模型的训练与评估}

\subsection{数据收集与预处理}

Before training a DQN model, it is important to preprocess the input data to make it suitable for the neural network.

This can include steps such as normalization, scaling, and cropping of images.

The RL agent also needs to collect data by interacting with the environment.

During this process, the agent chooses actions based on the current state, receives a reward, and transitions to the next state.

The state, action, reward, and next state are stored in a replay buffer, which is then used for training.

\subsection{模型训练}

To train a DQN model, the neural network is updated using stochastic gradient descent with mini-batches of state-action pairs from the replay buffer.

During each training iteration, the model predicts Q-values for the current state, and the Q-value for the chosen action is compared to the target Q-value, which is calculated using the Bellman equation.

The difference between the predicted and target Q-values is used to calculate the loss, which is then backpropagated through the network to update the weights.

To improve stability and convergence, various techniques such as experience replay, target networks, and prioritized experience replay can be used.

\subsection{模型的评估}

Once the DQN model is trained, it is evaluated to assess its performance.

The agent interacts with the environment using the trained model, and the cumulative reward obtained over a fixed number of episodes is used as a metric of performance.

The agent may also be tested on a holdout set of data to assess its generalization capabilities.

If the performance of the model is not satisfactory, the hyperparameters or model architecture can be adjusted, and the training and evaluation process can be repeated.