\chapter{深度强化学习}
\label{chp:initialization}

\section{强化学习}
\subsection{相关术语}
智能体：任何有独立思想并且可以与所处环境进行交互的实体。在交通场景下，智能体可以是行人，车辆，信号灯等。

状态：当前时刻智能体对周围环境的感知。所有时刻的感知集合构成了状态空间。

动作：智能体在当前状态下采取的行动。在当前状态下能采取的所有动作构成了动作空间。

策略：智能体根据当前时刻的状态应采取哪个动作的控制准则。在数学上的含义为，使用概率密度函数表示智能体在每个状态下采取各个动作的概率。

奖励：在智能体采取动作后，环境对智能体的反馈效果。奖励R可以为正反馈或负反馈。

回报：智能体从当前时刻至行动结束所能获得的累积奖励之和。

状态转移：当智能体采取动作后，由当前状态转移到下一个状态的过程。状态转移过程大多数具有随机性，该随机性来源于环境。
\subsection{基于价值的强化学习}

在强化学习中，任意t时刻的状态$s_t$下执行策略π中的动作$a_t$都存在一个对应的奖励$R_t$，由于强化学习所研究的问题具有马尔可夫性，因此系统的整体回报$U^t$与当前时刻的奖励$R_t$和未来时刻的奖励$R_(t+n)$有关，所以存在等式：

式中，是折减因子（discount factor）。

\subsection{基于策略的强化学习}

\subsection{价值与策略相结合的强化学习方法}

\section{深度学习}

\subsection{神经网络}

\subsection{激活函数}

\subsection{损失函数优化}

\section{深度强化学习}

\subsection{深度Q学习}

\subsection{近端策略优化}

\subsection{深度确定性策略梯度}
